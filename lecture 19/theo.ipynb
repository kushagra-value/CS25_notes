{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS25: V3 - Low-Level Embodied Intelligence with Foundation Models\n",
        "\n",
        "## Executive Summary & Learning Objectives\n",
        "\n",
        "Welcome! This notebook is your comprehensive guide to understanding how we can imbue robots with **low-level embodied intelligence** using the power of **Foundation Models**. Inspired by Fei Xia's lecture from Stanford's CS25 and the research paper \"A Survey on Robotics with Foundation Models: toward Embodied AI\" by Zhiyuan Xu et al. (2024), our goal is to transform you from a beginner into an expert on this topic. [1, 2]\n",
        "\n",
        "We will move beyond using models for just high-level planning (e.g., \"clean the kitchen\") and dive deep into the challenge of generating precise, low-level actions (e.g., \"move joint 3 by 0.5 radians\").\n",
        "\n",
        "### Learning Objectives:\n",
        "1.  **Understand the Core Challenge:** Grasp why low-level robotic control is fundamentally harder than high-level reasoning (Moravec's Paradox).\n",
        "2.  **Master the VLA Model Concept:** Learn how Vision-Language-Action (VLA) models like RT-2 work, treating robotic actions as a new \"language.\"\n",
        "3.  **Implement Action Tokenization:** Gain hands-on experience by implementing the action discretization scheme used in models like RT-1 and RT-2.\n",
        "4.  **Explore Alternative Interfaces:** Understand the novel concept of using \"Language to Reward\" as a powerful, alternative interface for controlling robots without direct action generation.\n",
        "5.  **Appreciate the Role of Data & Scaling:** Analyze the critical role of data scaling, positive transfer, and the challenges of robotic data collection (the Open X-Embodiment initiative).\n",
        "6.  **Connect to High-Level Planning:** See how these low-level control strategies fit into the broader picture of Embodied AI, linking back to the high-level planning taxonomies from the research paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites & The Fundamental Challenge of Embodied AI\n",
        "\n",
        "Before we build, we must understand the problem. Why is making a robot put a can in a sink so difficult?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 What is Embodied Intelligence?\n",
        "\n",
        "**Embodied Artificial Intelligence** refers to AI systems that can interact with the physical world through sensors (like cameras and touch sensors) and actuators (like motors and grippers). Unlike AI that lives purely on the internet (like a chatbot), embodied agents must understand and navigate the messy, unstructured, and dynamic physical world.\n",
        "\n",
        "The ultimate goal is to build **general-purpose robots** that can perform a wide variety of tasks in everyday environments, like our homes.\n",
        "\n",
        "**A Memorable Example: The Perils of the Physical World**\n",
        "\n",
        "Fei Xia's lecture gives us two perfect, unforgettable examples of why this is so hard:\n",
        "\n",
        "1.  **The Coke Can & The Faucet:** A robot is tasked with a simple command: \"put the Coke can in the sink.\" Instead of just placing it, the robot turns on the faucet, creating a potentially dangerous and messy situation. This wasn't a failure of understanding the goal, but a failure of understanding the **consequences of its actions**—it lacks a robust **world model**.\n",
        "    \n",
        "    *   **World Model:** An internal understanding of how the world works, including object properties (faucets release water), physics (water flows down), and the effects of actions.\n",
        "\n",
        "2.  **The Spilling Can:** A robot successfully grasps a can to throw it away. However, its pre-programmed \"tuck arm\" behavior flips the can upside down, which would spill any liquid inside and potentially damage the robot.\n",
        "    \n",
        "    *   This shows a lack of **situational awareness** and the rigidity of pre-programmed actions. The robot isn't reasoning about the can's contents or orientation.\n",
        "\n",
        "These examples vividly illustrate that embodied intelligence is not just about perception and action; it's about reasoning, prediction, and understanding the nuances of physical interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Moravec's Paradox: Why Robots Find Hard Things Easy and Easy Things Hard\n",
        "\n",
        "This is a core concept that explains the central difficulty in robotics.\n",
        "\n",
        "**The Paradox:** In AI and robotics, tasks that are **hard for humans** (like abstract reasoning, logic, playing chess, complex math) are relatively **easy for computers**. Conversely, tasks that are **easy for humans** (like perception, sensorimotor skills, walking, picking up an object) are incredibly **difficult for computers**.\n",
        "\n",
        "**Why? The Power of Evolution**\n",
        "*   **Human Sensorimotor Skills:** We have millions of years of evolution baked into our DNA. Our ability to see, walk, and manipulate objects is a highly optimized, pre-loaded \"operating system\" that we get for free. A toddler can effortlessly pick up a toy, a task that requires immense computation for a robot.\n",
        "*   **Computer Reasoning:** Abstract reasoning is a very recent development for humans. Computers, with their massive computational power, can excel at these logic-based tasks from a blank slate.\n",
        "\n",
        "**A Memorable Analogy: Kasparov's Defeat**\n",
        "When IBM's Deep Blue defeated Garry Kasparov in chess, the computer was performing superhuman reasoning. However, a human was still required to physically move the chess pieces on the board for the computer. Deep Blue could out-think the world champion but couldn't perform the simple physical task a child could.\n",
        "\n",
        "This paradox is why using Foundation Models (which are masters of reasoning and knowledge) for low-level physical control is such a fascinating and difficult challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 The Data Problem: Training Data Bias\n",
        "\n",
        "Foundation Models are trained on massive, internet-scale datasets. This creates a fundamental bias.\n",
        "\n",
        "**The \"WikiHow\" Bias:**\n",
        "*   **High-Level Knowledge is Abundant:** The internet is filled with text describing high-level, procedural tasks. There are countless WikiHow articles, blogs, and manuals explaining \"how to clean a kitchen\" or \"how to bake a cake.\"\n",
        "*   **Low-Level Knowledge is Scarce:** There are virtually **no** articles explaining \"how to move your finger 5 centimeters to the left with a specific velocity profile to grasp a cup.\" Humans don't describe their physical actions at this level of detail.\n",
        "\n",
        "This means that while a Foundation Model has a vast prior for high-level planning, it has almost no prior knowledge for low-level motor control. This leads to the two core challenges Fei Xia identifies:\n",
        "1.  **Lack of Robotic Data:** It is incredibly slow and expensive to collect real-world robot demonstration data compared to scraping text and images from the web.\n",
        "2.  **Lack of a Low-Level Interface:** LLMs don't natively \"speak robot.\" They can't output the raw joint angles or motor torques needed for control.\n",
        "\n",
        "Our journey in this notebook is to explore how researchers are tackling these two fundamental problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Consolidation & The Rise of VLA Models\n",
        "\n",
        "The first major approach to solving the low-level control problem is to reframe it entirely. Instead of creating a separate interface, what if we could teach a Foundation Model to **treat robot actions as just another language**? This leads to the idea of Vision-Language-Action (VLA) models.\n",
        "\n",
        "### 2.1 The Evolution: From Separate Models to a Unified System\n",
        "\n",
        "Fei Xia presents a clear evolutionary path of robotic systems at Google DeepMind, showing a trend towards **model consolidation**.\n",
        "\n",
        "**Interactive Visualization: The Path to RT-2**\n",
        "\n",
        "Imagine a diagram with four stages. As you click on each stage, its components are revealed.\n",
        "\n",
        "*   **Stage 1: SayCan (2022)**\n",
        "    *   **Planning:** Large Language Model (e.g., PaLM)\n",
        "    *   **Affordance (What's possible?):** Separate Vision Model (e.g., TQ-Opt)\n",
        "    *   **Low-Level Policy (How to act?):** Separate Robotics Model (e.g., RT-1)\n",
        "    *   *Insight:* A system of separate, specialized experts that need to be trained and integrated individually.\n",
        "\n",
        "*   **Stage 2: Q-Transformer (2023)**\n",
        "    *   **Planning:** Large Language Model\n",
        "    *   **Affordance + Low-Level Policy:** Unified Transformer-based policy (trained with offline RL)\n",
        "    *   *Insight:* The low-level skills and the understanding of what's possible are unified, simplifying the system.\n",
        "\n",
        "*   **Stage 3: PaLM-E (2023)**\n",
        "    *   **Planning + Affordance:** Unified Embodied VLM (PaLM-E)\n",
        "    *   **Low-Level Policy:** Separate Robotics Model (e.g., RT-1)\n",
        "    *   *Insight:* The high-level reasoning and environmental understanding are now in a single, powerful model. This model can take in multimodal inputs (text, images, sensor data).\n",
        "\n",
        "*   **Stage 4: RT-2 (2023)**\n",
        "    *   **Planning + Affordance + Low-Level Policy:** **A single, end-to-end Vision-Language-Action Model!**\n",
        "    *   *Insight:* The ultimate consolidation. One model takes in vision and language, and outputs actions directly. This is the core breakthrough we will explore.\n",
        "\n",
        "This trend shows that the field is moving towards building a single, universal model that can handle everything from high-level reasoning to low-level control, unified by the common language of text and tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Deep Dive: RT-2 - Teaching a VLM to Speak Robot\n",
        "\n",
        "Robotic Transformer 2 (RT-2) is a paradigm-shifting model. It shows that a pre-trained Vision-Language Model (VLM) can be repurposed for robotic control, and in doing so, it transfers its vast \"web knowledge\" to the physical world.\n",
        "\n",
        "**The Core Idea:**\n",
        "RT-2's central hypothesis is that we can represent robot actions as text tokens and then fine-tune a VLM to generate these action tokens just like it would generate words in a sentence.\n",
        "\n",
        "**Input:** Image + Text Instruction (e.g., \"Pick up the extinct animal\")\n",
        "**Output:** A string of text that represents a robot action.\n",
        "\n",
        "Let's break down the most critical component: **Action Representation**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- The Most Important Concept: Action Tokenization ---\n",
        "# Let's implement the action representation from RT-1, used by RT-2.\n",
        "\n",
        "# The robot's action space has 8 dimensions:\n",
        "# - 3D change in end-effector position (dx, dy, dz)\n",
        "# - 3D change in end-effector rotation (d_roll, d_pitch, d_yaw)\n",
        "# - 1D gripper state (open/closed)\n",
        "# - 1D episode termination (continue/terminate)\n",
        "\n",
        "def discretize_action(action_continuous, num_bins=256):\n",
        "    \"\"\"Converts a continuous action vector into a discretized token vector.\"\"\"\n",
        "    # action_continuous is a dictionary with continuous values\n",
        "    # e.g., {'dx': 0.05, 'dy': -0.02, ...}\n",
        "    \n",
        "    # Values are assumed to be normalized between -1 and 1 (except for termination)\n",
        "    # We map this range to [0, num_bins-1]\n",
        "    \n",
        "    discretized = {}\n",
        "    \n",
        "    # Position and Rotation (6 DoF)\n",
        "    for key in ['dx', 'dy', 'dz', 'd_roll', 'd_pitch', 'd_yaw']:\n",
        "        # Clamp to ensure it's within [-1, 1]\n",
        "        val = np.clip(action_continuous[key], -1.0, 1.0)\n",
        "        # Normalize from [-1, 1] to [0, 1]\n",
        "        normalized_val = (val + 1.0) / 2.0\n",
        "        # Scale to bin index and convert to integer\n",
        "        bin_index = int(normalized_val * (num_bins - 1))\n",
        "        discretized[key] = bin_index\n",
        "        \n",
        "    # Gripper State\n",
        "    # Let's say -1 is fully open, 1 is fully closed\n",
        "    gripper_val = np.clip(action_continuous['gripper'], -1.0, 1.0)\n",
        "    normalized_gripper = (gripper_val + 1.0) / 2.0\n",
        "    discretized['gripper'] = int(normalized_gripper * (num_bins - 1))\n",
        "    \n",
        "    # Termination\n",
        "    # Simple binary: 0 for continue, 1 for terminate\n",
        "    discretized['terminate'] = 1 if action_continuous['terminate'] > 0 else 0\n",
        "    \n",
        "    return discretized\n",
        "\n",
        "def action_to_string(discretized_action):\n",
        "    \"\"\"Converts the discretized action dictionary into the final string format.\"\"\"\n",
        "    action_order = ['dx', 'dy', 'dz', 'd_roll', 'd_pitch', 'd_yaw', 'gripper', 'terminate']\n",
        "    # The format is a space-separated string of the integer tokens\n",
        "    action_str = \" \".join([str(discretized_action[key]) for key in action_order])\n",
        "    return action_str\n",
        "\n",
        "def detokenize_string(action_str, num_bins=256):\n",
        "    \"\"\"Converts the string back into a continuous action dictionary.\"\"\"\n",
        "    tokens = [int(t) for t in action_str.split()]\n",
        "    keys = ['dx', 'dy', 'dz', 'd_roll', 'd_pitch', 'd_yaw', 'gripper', 'terminate']\n",
        "    continuous_action = {}\n",
        "    \n",
        "    # First 7 dimensions\n",
        "    for i in range(7):\n",
        "        token = tokens[i]\n",
        "        # Normalize from [0, 255] to [0, 1]\n",
        "        normalized_val = token / (num_bins - 1)\n",
        "        # Denormalize from [0, 1] to [-1, 1]\n",
        "        val = (normalized_val * 2.0) - 1.0\n",
        "        continuous_action[keys[i]] = val\n",
        "        \n",
        "    # Termination\n",
        "    continuous_action['terminate'] = 1 if tokens[7] > 0 else 0\n",
        "    return continuous_action\n",
        "\n",
        "# --- Example 1: Pushing forward ---\n",
        "action_push_forward = {\n",
        "    'dx': 0.1, 'dy': 0.0, 'dz': 0.0,  # Move forward slightly in x\n",
        "    'd_roll': 0, 'd_pitch': 0, 'd_yaw': 0, # No rotation\n",
        "    'gripper': -1, # Keep gripper open\n",
        "    'terminate': 0 # Continue episode\n",
        "}\n",
        "\n",
        "discretized_1 = discretize_action(action_push_forward)\n",
        "string_1 = action_to_string(discretized_1)\n",
        "\n",
        "print(\"--- Example 1: Push Forward ---\")\n",
        "print(f\"Continuous Action: {action_push_forward}\")\n",
        "print(f\"Discretized Tokens: {discretized_1}\")\n",
        "print(f\"Final Action String (This is what the LLM outputs!): '{string_1}'\\n\")\n",
        "\n",
        "# --- Example 2: Grasping and moving up ---\n",
        "action_grasp_up = {\n",
        "    'dx': 0.0, 'dy': 0.0, 'dz': 0.2, # Move up slightly in z\n",
        "    'd_roll': 0, 'd_pitch': 0, 'd_yaw': 0, # No rotation\n",
        "    'gripper': 1, # Close gripper\n",
        "    'terminate': 0 # Continue episode\n",
        "}\n",
        "\n",
        "discretized_2 = discretize_action(action_grasp_up)\n",
        "string_2 = action_to_string(discretized_2)\n",
        "\n",
        "print(\"--- Example 2: Grasp and Move Up ---\")\n",
        "print(f\"Continuous Action: {action_grasp_up}\")\n",
        "print(f\"Discretized Tokens: {discretized_2}\")\n",
        "print(f\"Final Action String: '{string_2}'\\n\")\n",
        "\n",
        "# --- Example 3: Terminating the episode ---\n",
        "action_terminate = {\n",
        "    'dx': 0.0, 'dy': 0.0, 'dz': 0.0, \n",
        "    'd_roll': 0, 'd_pitch': 0, 'd_yaw': 0, \n",
        "    'gripper': 1, # Gripper state doesn't matter much\n",
        "    'terminate': 1 # Terminate episode\n",
        "}\n",
        "\n",
        "discretized_3 = discretize_action(action_terminate)\n",
        "string_3 = action_to_string(discretized_3)\n",
        "print(\"--- Example 3: Terminate Episode ---\")\n",
        "print(f\"Final Action String: '{string_3}'\\n\")\n",
        "\n",
        "# --- Let's test the round trip ---\n",
        "print(\"--- Testing Detokenization ---\")\n",
        "reconstructed_action_1 = detokenize_string(string_1)\n",
        "print(f\"Original Continuous Action 1: {action_push_forward}\")\n",
        "print(f\"Reconstructed Action 1: {{k: round(v, 2) for k, v in reconstructed_action_1.items()}}\")\n",
        "print(\"Note: Small differences due to quantization error are expected.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Training Process (Co-fine-tuning):**\n",
        "\n",
        "The magic of RT-2 comes from how it's trained. You don't just train it on robot data, as that would cause it to forget its vast web knowledge (a problem called **catastrophic forgetting**).\n",
        "\n",
        "Instead, you **co-fine-tune** it on a mixture of data:\n",
        "1.  **Internet-Scale Vision-Language Data:** Standard datasets for tasks like visual question answering (VQA), image captioning, etc. (e.g., \"Image: [a cat on a mat], Text: What is on the mat? -> A cat\")\n",
        "2.  **Robotics Data:** The RT-1 dataset, where each example is (Image, Instruction, Action String). (e.g., \"Image: [a can on a table], Instruction: 'pick up the can', Action: '128 128 166 128 128 128 255 0'\")\n",
        "\n",
        "By mixing these datasets, the model learns to perform the new robotics task while retaining its powerful, general-purpose reasoning and recognition capabilities from the web data. The web data acts as a powerful **regularizer**.\n",
        "\n",
        "### 2.3 Emergent Capabilities & Positive Transfer\n",
        "\n",
        "The results are astonishing. Because RT-2 retains its web-scale knowledge, it can perform tasks it was **never explicitly trained on** in the robotics dataset. This is called **emergent generalization**.\n",
        "\n",
        "**Memorable Example: \"Pick up the extinct animal\"**\n",
        "*   **The Task:** The robot sees a table with various toys, including a plastic dinosaur.\n",
        "*   **RT-1 (Trained on robotics data only):** Would fail. It doesn't know what \"extinct animal\" means and cannot link it to the dinosaur toy.\n",
        "*   **RT-2 (Co-fine-tuned):**\n",
        "    1.  **Vision-Language Knowledge:** From its web pre-training, it understands the concept \"extinct animal\" and can visually identify the toy dinosaur as an instance of that concept.\n",
        "    2.  **Robotics Knowledge:** From its robotics fine-tuning, it knows how to generate the action sequence to physically pick up an object at the dinosaur's location.\n",
        "    3.  **Result:** It correctly picks up the dinosaur.\n",
        "\n",
        "This is a profound demonstration of **positive transfer**: knowledge from the vision-language domain directly improves performance and enables new capabilities in the robotics domain.\n",
        "\n",
        "**Chain-of-Thought Reasoning for Actions:**\n",
        "The model can even be prompted to \"think\" before it acts. Instead of directly outputting an action string, it can be prompted to first generate a natural language plan, and then the action string.\n",
        "\n",
        "*   **Instruction:** \"I need to hammer a nail. Which object might be useful?\"\n",
        "*   **Scene:** A rock, headphones, a sticky note.\n",
        "*   **RT-2 Chain-of-Thought Output:** \"Plan: The rock can be used as a hammer. Action: [action string to pick up the rock]\"\n",
        "\n",
        "This shows a deeper level of reasoning being directly integrated into the action-generation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Language to Reward: A New Interface for Low-Level Control\n",
        "\n",
        "While VLA models like RT-2 are powerful, they have limitations. The generated actions are constrained by the kind of data they were trained on. It's hard for them to produce novel, highly dynamic, or physically optimized motions.\n",
        "\n",
        "An alternative approach, also explored in the lecture, is to use language models for what they do best—high-level semantic understanding—and offload the low-level control to a dedicated optimizer. The bridge between them is **reward functions**.\n",
        "\n",
        "### 3.1 The Core Idea: Language Models as Reward Coders\n",
        "\n",
        "Instead of asking an LLM to generate an action, we ask it to **generate a Python function that calculates a reward** for a given state. This reward function mathematically describes the goal.\n",
        "\n",
        "**Analogy: The Architect and The Builder**\n",
        "*   **Direct Action Generation (RT-2):** This is like an architect who tries to also be the builder, laying every single brick themselves. They are limited by their personal brick-laying experience.\n",
        "*   **Language to Reward:** This is like an architect who creates a detailed blueprint (the reward function) that specifies the goal (e.g., \"the wall must be 10 feet tall, perfectly vertical, and use these specific materials\"). They then hand this blueprint to a team of expert builders (a trajectory optimizer) who figure out the best way to lay the bricks to satisfy the blueprint.\n",
        "\n",
        "This decouples the *what* (the goal, specified by the LLM) from the *how* (the specific motor actions, figured out by the optimizer).\n",
        "\n",
        "### 3.2 The Two-Stage Prompting Process\n",
        "\n",
        "Directly generating reward code can still be hard. The lecture proposes a clever two-stage process to improve results:\n",
        "1.  **Stage 1: Generate Motion Description:** First, prompt the LLM to generate a rich, descriptive natural language explanation of the desired motion. This gets the model to \"think\" about the problem in its native domain.\n",
        "2.  **Stage 2: Generate Reward Code:** In a second step, provide the model with its own motion description and ask it to translate that description into a reward function.\n",
        "\n",
        "This intermediate representation helps bridge the gap between abstract commands and structured code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Conceptual Implementation: Language to Reward ---\n",
        "\n",
        "def mock_llm_call(prompt):\n",
        "    \"\"\"This is a mock function to simulate calling a large language model.\"\"\"\n",
        "    print(\"--- LLM PROMPT ---\")\n",
        "    print(prompt)\n",
        "    print(\"--------------------\\n\")\n",
        "    # In a real scenario, this would be an API call to an LLM.\n",
        "    # We will return hardcoded responses for this example.\n",
        "    if \"Describe the motion\" in prompt:\n",
        "        return \"The robot should keep its torso upright and balanced. It should lift its front two feet off the ground, standing only on its two hind feet. The height of its torso should be maximized.\"\n",
        "    elif \"Translate the motion description into a reward function\" in prompt:\n",
        "        reward_code = \"\"\"\n",
        "def calculate_reward(state):\n",
        "    reward = 0\n",
        "    # 1. Torso upright: Penalize deviation from vertical alignment\n",
        "    torso_up_vector = state.get_torso_up_vector()\n",
        "    vertical_vector = [0, 0, 1]\n",
        "    # Dot product is max (1) when vectors are aligned\n",
        "    reward += torso_up_vector.dot(vertical_vector)\n",
        "    \n",
        "    # 2. Front feet off the ground: Reward for height of front feet\n",
        "    front_left_foot_height = state.get_foot_height('front_left')\n",
        "    front_right_foot_height = state.get_foot_height('front_right')\n",
        "    reward += (front_left_foot_height + front_right_foot_height) * 2.0\n",
        "    \n",
        "    # 3. Maximize torso height\n",
        "    torso_height = state.get_torso_height()\n",
        "    reward += torso_height * 1.5\n",
        "    \n",
        "    # 4. Penalize effort to encourage efficiency\n",
        "    control_effort = state.get_control_effort()\n",
        "    reward -= 0.01 * control_effort\n",
        "    \n",
        "    return reward\n",
        "\"\"\"\n",
        "        return reward_code\n",
        "    return \"\"\n",
        "\n",
        "# 1. User gives a high-level command\n",
        "user_command = \"Make the robot dog stand up on two feet like a human.\"\n",
        "print(f\"USER COMMAND: {user_command}\\n\")\n",
        "\n",
        "# 2. Stage 1: Generate Motion Description\n",
        "motion_description_prompt = f\"\"\"\n",
        "Task: {user_command}\n",
        "Describe the motion in detail, focusing on key physical attributes.\n",
        "\"\"\"\n",
        "motion_description = mock_llm_call(motion_description_prompt)\n",
        "print(\"--- LLM RESPONSE (Motion Description) ---\")\n",
        "print(motion_description + \"\\n\")\n",
        "\n",
        "# 3. Stage 2: Generate Reward Code\n",
        "# In a real system, you would also provide the available state variables (the API)\n",
        "reward_code_prompt = f\"\"\"\n",
        "Translate the motion description into a Python reward function.\n",
        "\n",
        "Motion Description: '{motion_description}'\n",
        "\n",
        "Available state functions:\n",
        "- state.get_torso_up_vector()\n",
        "- state.get_foot_height(foot_name)\n",
        "- state.get_torso_height()\n",
        "- state.get_control_effort()\n",
        "\n",
        "def calculate_reward(state):\n",
        "    # ... your code here\n",
        "\"\"\"\n",
        "reward_code = mock_llm_call(reward_code_prompt)\n",
        "print(\"--- LLM RESPONSE (Reward Code) ---\")\n",
        "print(reward_code)\n",
        "\n",
        "print(\"This generated code can now be used by a trajectory optimizer (like MPC) to find the best actions.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Coaching the Robot: Iterative Refinement\n",
        "\n",
        "A major advantage of the Language-to-Reward approach is that it's highly interactive and correctable. If the robot's motion isn't quite right, you don't need to retrain the model; you just correct it with more specific language.\n",
        "\n",
        "**Memorable Example: Teaching a Robot the Moonwalk**\n",
        "1.  **Initial Command:** \"Make the robot do a moonwalk while standing up.\"\n",
        "2.  **Initial Result:** The robot might just walk backward awkwardly. The LLM has a vague idea but misses the nuance.\n",
        "3.  **The Correction (The Coaching):** The user provides feedback:\n",
        "    *   `\"You were close, but a moonwalk means the robot should walk backwards WHILE its feet swing AS IF they are moving forwards. Also, try to make it move at 0.5 m/s.\"`\n",
        "4.  **Refined Result:** This more detailed prompt is given back to the LLM. It updates its internal motion description and generates a new, more sophisticated reward function that captures the essence of the moonwalk (e.g., by adding terms that reward backward torso velocity while also rewarding forward foot swing velocity relative to the torso).\n",
        "\n",
        "This ability to coach and refine behavior through natural language is a powerful paradigm for human-robot interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Datasets, Benchmarks, and the Challenge of Scale\n",
        "\n",
        "The success of foundation models is built on data. In robotics, data is both the biggest opportunity and the biggest bottleneck.\n",
        "\n",
        "### 4.1 The Great Data Discrepancy\n",
        "\n",
        "Let's visualize the scale difference mentioned in the lecture and paper. [1]\n",
        "\n",
        "**Interactive Visualization: Data Scale Comparison**\n",
        "\n",
        "Imagine a bar chart where the y-axis is on a logarithmic scale.\n",
        "\n",
        "*   **Bar 1: LAION-5B (Vision-Language Dataset):** Height corresponds to **5,750,000,000** text-image pairs.\n",
        "*   **Bar 2: RT-1 (Robotics Dataset):** Height corresponds to **130,000** episodes.\n",
        "*   **Bar 3: VIMA (Robotics Dataset):** Height corresponds to **650,000** demonstrations.\n",
        "\n",
        "The LAION-5B bar would be orders of magnitude taller, vividly illustrating the data scarcity in robotics. Collecting 130k episodes for RT-1 took **13 robots 17 months** of continuous effort. Collecting billions of robotics episodes with this method is simply impossible.\n",
        "\n",
        "### 4.2 Solutions and Strategies\n",
        "\n",
        "Given this challenge, the community is exploring several avenues:\n",
        "\n",
        "1.  **Scaling Collaboration (Open X-Embodiment - RTX):**\n",
        "    *   As described in the lecture, this is a massive effort to pool robotics data from many different labs and institutions.\n",
        "    *   It combines data from **22 different robot types** to study **cross-embodiment transfer**—can a model trained on a Franka arm and a Sawyer arm also control a new, unseen robot type?\n",
        "    *   Early results show that the jointly trained RTX model outperforms models trained on single-robot datasets, proving there are significant benefits to data diversity.\n",
        "\n",
        "2.  **Utilizing Human Data:**\n",
        "    *   The research paper mentions using large-scale human activity datasets (like Ego4D). [1] These are videos of humans performing tasks from a first-person perspective.\n",
        "    *   **Advantage:** Easy to collect (download from the internet).\n",
        "    *   **Challenge:** The **morphology gap**. Humans and robots have very different bodies. Aligning human motions to a robot's capabilities is a difficult research problem.\n",
        "\n",
        "3.  **High-Fidelity Simulation (Sim2Real):**\n",
        "    *   Train agents in a highly realistic simulator where data collection is cheap and fast, and then transfer the learned policy to the real world.\n",
        "    *   **Challenge:** The **reality gap**. Even the best simulators can't perfectly capture the physics and appearance of the real world. Overcoming this gap is key.\n",
        "\n",
        "4.  **Data Augmentation (ROSIE):**\n",
        "    *   The research paper mentions ROSIE, a method that uses generative models (diffusion) to augment existing robotics data. [1] For example, it can take an image from a demonstration, use a VLM to identify the task-relevant object (e.g., the 'apple'), and then use an image editor like Imagen to change the background or add distractors, creating a new training example without destroying the core semantic information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Future Directions\n",
        "\n",
        "We've journeyed deep into the world of low-level embodied intelligence, bridging the gap between abstract language and concrete physical action.\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1.  **The Challenge is Real:** Moravec's paradox and the inherent data bias of LLMs make low-level control the hardest part of embodied AI.\n",
        "2.  **Actions can be a Language (VLA Models):** By tokenizing robot actions, we can fine-tune powerful VLMs like RT-2 to directly output control commands, transferring web-scale knowledge to physical tasks.\n",
        "3.  **Rewards can be an Interface:** By using LLMs to generate reward functions, we can leverage their semantic understanding while offloading complex motion optimization to specialized controllers. This approach is highly flexible and interactive.\n",
        "4.  **Data is the Ultimate Driver (and Bottleneck):** Progress is inextricably linked to data. Scaling robotic datasets through collaboration (RTX), simulation, and clever augmentation is crucial for the future.\n",
        "5.  **Positive Transfer is Everywhere:** The most exciting finding is that knowledge is transferable. Training on diverse data—whether it's web text, images, or data from different robots—consistently leads to more capable and generalizable models.\n",
        "\n",
        "### Future Directions (Synthesized from Lecture & Paper):\n",
        "\n",
        "*   **Synergies of Planning and Control:** Developing more seamless models that can simultaneously learn high-level plans and low-level skills, avoiding the accumulation of errors in long-horizon tasks.\n",
        "*   **Tackling Hallucination & Safety:** A critical frontier. When a model that controls a physical robot hallucinates (e.g., GPT-4V sometimes \"sees\" objects that aren't there), the consequences can be catastrophic. Developing robust safety protocols, supervisors, and failure recovery strategies is imperative. [1]\n",
        "*   **Efficiency and Deployment:** Making these huge models faster and smaller so they can be deployed on resource-constrained robots, rather than relying on massive, off-board TPU clusters.\n",
        "*   **Finding the Right Representation:** The quest for the perfect action representation and the best interface between language and control is far from over. Is it discretized tokens? Is it reward functions? Is it something else entirely?\n",
        "\n",
        "The field of embodied AI is at an inflection point. By cleverly combining the reasoning power of foundation models with the physical realities of robotics, we are moving towards a new era of more general, capable, and intelligent agents."
      ]
    }
  ]
}