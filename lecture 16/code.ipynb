{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS25: Common Sense Reasoning - Comprehensive Notes\n",
        "\n",
        "These notes provide a detailed overview of the Stanford CS25 lecture on Common Sense Reasoning. The goal is to provide a self-sufficient resource.  We'll cover the key topics, examples, and research directions discussed, enhanced with visualizations and Python code to illustrate concepts interactively. This notebook aims to provide an in-depth understanding of the concepts presented.\n",
        "\n",
        "**Table of Contents:**\n",
        "1.  Introduction: The Illusion of Solved Common Sense\n",
        "2.  Maieutic Prompting: Socratic Reasoning for LMs\n",
        "    *   Maieutic Inference Tree\n",
        "    *   Logical Consistency\n",
        "    *   Belief and Pairwise Consistency\n",
        "    *   Constraint Optimization\n",
        "    *   Empirical Results\n",
        "3.  Symbolic Knowledge Distillation: From Language Models to Causal Commonsense Models\n",
        "    *   Systematic Generalization Problem\n",
        "    *   Commonsense Definition\n",
        "    *   ATOMIC Knowledge Graph\n",
        "    *   Symbolic Knowledge Distillation Process\n",
        "    *   Critique Model\n",
        "    *   Results: Machine-Authored vs. Human-Authored Knowledge\n",
        "4.  Commonsense Morality: Aligning AI with Human Values\n",
        "    *   Moral Implications of Language Models\n",
        "    *   Delphi: A Commonsense Moral Model\n",
        "    *   Commonsense Norm Bank\n",
        "    *   Social Chemistry and Bias Reframing\n",
        "    *   Delphi Hybrid: Neural-Symbolic Reasoning\n",
        "    *   AI Safety, Equity, and Morality\n",
        "5.  Open Questions and Future Directions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction: The Illusion of Solved Common Sense\n",
        "\n",
        "The lecture begins by addressing the common question of whether large language models (LLMs) like ChatGPT have solved common sense reasoning.  While LLMs can perform impressively on certain tasks, they often exhibit unreliability and inconsistencies when challenged with slightly modified or adversarial examples. The speaker uses the Winograd Schema Challenge as an example, showing that ChatGPT can answer a question correctly but fail when the wording is subtly altered.\n",
        "\n",
        "**Example:**\n",
        "*   Original: \"The trophy doesn't fit in the brown suitcase because it's too big. What's too big?\"\n",
        "*   ChatGPT (Correct): \"The trophy\"\n",
        "*   Modified: \"The trophy doesn't fit in the brown suitcase because it's too small. What's too small?\"\n",
        "*   ChatGPT (Incorrect): \"The trophy\"\n",
        "\n",
        "This highlights that while LLMs may appear to possess common sense, their reasoning is often superficial and easily disrupted.\n",
        "\n",
        "The lecture emphasizes the importance of rigorous evaluation, focusing on new tasks and leaderboards, and innovating algorithms and data to improve the true common sense capabilities of AI models. Key themes are that smaller models can be better with better data and algorithms, and that *knowledge is power*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Maieutic Prompting: Socratic Reasoning for LMs\n",
        "\n",
        "Maieutic prompting is a technique inspired by Socrates' method of questioning to improve the logical consistency of LLMs. The central idea is to build a *Maieutic inference tree* by recursively prompting the LLM to explain its reasoning and then evaluating the consistency of those explanations.\n",
        "\n",
        "### Maieutic Inference Tree\n",
        "\n",
        "The tree is constructed as follows:\n",
        "\n",
        "1.  **Start with a question and a proposed answer (True or False).**\n",
        "2.  **Prompt the LLM to explain *why* the answer is True (Explanation of True, E(T)).**\n",
        "3.  **Prompt the LLM to explain *why* the answer is False (Explanation of False, E(F)).**\n",
        "4.  **Check the logical consistency of the explanations.** For E(T), ask the LLM if it agrees with the explanation.  For E(F), ask if it disagrees (negates) with the explanation.\n",
        "5.  **Recursively repeat steps 2-4 on the explanations themselves, building a tree of reasoning.**\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*   Question: \"If you travel West far enough from the West Coast, you will reach the East Coast or not?\"\n",
        "*   Proposed Answer: True\n",
        "*   E(T): \"The world is round, so eventually you will reach the East Coast.\"\n",
        "*   E(F): \"You cannot reach the East Coast because the world is flat.\"\n",
        "\n",
        "### Logical Consistency\n",
        "\n",
        "The goal is to identify and prune branches of the tree where the LLM exhibits logical inconsistencies.  If the LLM contradicts its own explanations, that branch is considered unreliable and discarded. The speaker emphasizes how language models can be inconsistent with their own statements.\n",
        "\n",
        "### Belief and Pairwise Consistency\n",
        "\n",
        "Even after pruning inconsistent branches, the remaining tree may still contain inconsistencies. To address this, the following steps are taken:\n",
        "\n",
        "1.  **Compute Node-wise Confidence (Belief):** Calculate a confidence score for each node based on conditional probabilities. The speaker describes the equation as looking at different conditional probabilities and then computes its ratio to see how confident it is for any particular node.\n",
        "2.  **Compute Edge-wise Consistency:** Use a Natural Language Inference (NLI) model to determine whether pairs of nodes are contradictory. This generates pairwise weights.\n",
        "\n",
        "### Constraint Optimization\n",
        "\n",
        "Finally, a constraint optimization problem is formulated to assign labels (True or False) to each node in the tree such that the overall consistency and confidence are maximized. This is solved using a Max-SAT solver. This process might flip the original label if it enhances graph-level consistency.\n",
        "\n",
        "### Empirical Results\n",
        "\n",
        "The speaker mentions improved performance on CommonsenseQA 2.0, CREAK, and Come2Sense benchmarks when using Maieutic prompting compared to other prompting methods like chain-of-thought or self-consistency. Maieutic prompting even outperforms supervised models trained on T5, suggesting the power of this inference-time algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E(T): Because the Earth is round.\n",
            "E(F): Because you'll fall off the edge.\n",
            "Consistency True: Yes\n",
            "Consistency False: No\n"
          ]
        }
      ],
      "source": [
        "# Example: Implementing a simplified Maieutic Prompting (Conceptual)\n",
        "\n",
        "def get_llm_response(question, model=\"GPT-3\"):  # Placeholder for LLM interaction\n",
        "    # Simulate getting response from an LLM\n",
        "    if question.startswith(\"Why is it True that\"):  # Basic example\n",
        "        if \"travel West\" in question:\n",
        "            return \"Because the Earth is round.\"\n",
        "        elif \"butterflies fly\" in question:\n",
        "            return \"Because they have four wings.\"\n",
        "    elif question.startswith(\"Why is it False that\"): # Example reasoning\n",
        "        if \"travel West\" in question:\n",
        "            return \"Because you'll fall off the edge.\"\n",
        "        elif \"butterflies fly\" in question:\n",
        "            return \"Because butterflies can't fly.\"\n",
        "    elif question.startswith(\"Does the following support True:\"):\n",
        "        if \"Earth is round\" in question:\n",
        "            return \"Yes\"\n",
        "        else:\n",
        "            return \"No\"\n",
        "    elif question.startswith(\"Does the following support False:\"):\n",
        "            return \"No\"\n",
        "    return \"\"\n",
        "\n",
        "def maieutic_prompting(question, answer):\n",
        "    # Step 1 & 2: E(T)\n",
        "    explanation_true = get_llm_response(f\"Why is it True that {question}?\", model=\"GPT-3\")\n",
        "    print(f\"E(T): {explanation_true}\")\n",
        "    # Step 3: E(F)\n",
        "    explanation_false = get_llm_response(f\"Why is it False that {question}?\", model=\"GPT-3\")\n",
        "    print(f\"E(F): {explanation_false}\")\n",
        "    # Step 4: Check Logical Consistency (simplified)\n",
        "    consistency_true = get_llm_response(f\"Does the following support True: {explanation_true}?\", model=\"GPT-3\")\n",
        "    consistency_false = get_llm_response(f\"Does the following support False: {explanation_false}?\", model=\"GPT-3\")\n",
        "    print(f\"Consistency True: {consistency_true}\")\n",
        "    print(f\"Consistency False: {consistency_false}\")\n",
        "    # Further steps (recursion, confidence, etc.) would be implemented here\n",
        "\n",
        "# Example Usage\n",
        "question = \"If you travel West far enough from the West Coast, you will reach the East Coast or not?\"\n",
        "maieutic_prompting(question, \"True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This simplified Python code provides a conceptual outline of Maieutic Prompting. It showcases how the initial explanations are generated and how logical consistency can be verified. Further development can extend this framework to realize a complete Maieutic Inference Tree with sophisticated confidence and consistency checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Symbolic Knowledge Distillation: From Language Models to Causal Commonsense Models\n",
        "\n",
        "The lecture then transitions to the topic of symbolic knowledge distillation, a technique to convert general language models into causal commonsense models. The motivation behind this is that despite impressive performance on leaderboards, state-of-the-art models are often brittle and make strange mistakes when faced with adversarial or out-of-domain examples.  They tend to learn surface patterns rather than true understanding.\n",
        "\n",
        "### Systematic Generalization Problem\n",
        "\n",
        "This brittleness is referred to as the *systematic generalization problem*. Unlike humans who conceptually understand how the world works, transformers learn surface patterns that are powerful but not robust.\n",
        "\n",
        "### Commonsense Definition\n",
        "\n",
        "The lecture defines commonsense as the basic level of practical knowledge and reasoning concerning everyday situations and events that are *commonly shared* among most people.  It emphasizes that commonsense is not universal knowledge but is shared across a large population, acknowledging that additional context can change what is considered commonsensical.\n",
        "\n",
        "**Example:**\n",
        "*   \"It's OK to keep the closet door open.\"\n",
        "*   \"It's NOT OK to keep the fridge door open (because the food might go bad).\"\n",
        "\n",
        "However, the speaker notes exceptions to the rules (e.g., fridge in a store, friend's house rules).\n",
        "\n",
        "### ATOMIC Knowledge Graph\n",
        "\n",
        "The speaker introduces ATOMIC, a symbolic commonsense knowledge graph, which was initially fully crowdsourced.  ATOMIC contains *if-then* knowledge about social interactions and physical entities.  For example, \"If X gets their car repaired,\" then:\n",
        "\n",
        "*   Effect: \"X might call Uber/Lyft.\"\n",
        "*   Need: \"X needs a mechanic and money.\"\n",
        "\n",
        "ATOMIC also captures physical entity-centric knowledge and counterfactual conditions.\n",
        "\n",
        "### Symbolic Knowledge Distillation Process\n",
        "\n",
        "The core idea of symbolic knowledge distillation is to distill knowledge from a large language model (teacher) into a smaller, more focused commonsense model (student). The goal is to create a student model that is *better* than the teacher, even though standard knowledge distillation often results in a worse model.\n",
        "\n",
        "The speaker introduces a *funnel* concept in the process. \n",
        "\n",
        "### Critique Model\n",
        "\n",
        "The *critic* plays a crucial role in symbolic knowledge distillation. A RoBERTa model is used as a critic to evaluate the quality of the knowledge generated by the teacher (GPT-3). The critic is trained on a labeled dataset to identify whether machine-generated knowledge is correct or not. The critic aggressively filters out bad knowledge, even if it means discarding some good knowledge. By filtering aggressively, the overall accuracy and diversity of the distilled knowledge is improved.\n",
        "\n",
        "### Results: Machine-Authored vs. Human-Authored Knowledge\n",
        "\n",
        "The surprising finding is that machine-authored knowledge, when filtered by the critic, can be *better* than human-authored knowledge in terms of scale, accuracy, and diversity. This is attributed to the combination of GPT-3's broad knowledge and RoBERTa's ability to filter out inconsistencies. The distilled knowledge is then used to train a downstream neural commonsense model, achieving performance close to 90% on causal reasoning tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distilled Knowledge:\n",
            "- Person calls a taxi.\n",
            "- Person calls a taxi.\n",
            "- Person pays the bill.\n",
            "- Person pays the bill.\n",
            "- Person calls a taxi.\n",
            "- Person pays the bill.\n"
          ]
        }
      ],
      "source": [
        "# Simplified example of Knowledge Distillation with a Critic (Conceptual)\n",
        "\n",
        "import random\n",
        "\n",
        "def teacher_generate(prompt):\n",
        "    \"\"\"Simulates a Teacher Language Model generating knowledge.\"\"\"\n",
        "    if \"car repaired\" in prompt:\n",
        "        options = [\n",
        "            \"Person calls a taxi.\",\n",
        "            \"Person pays the bill.\",\n",
        "            \"Person folds the car into origami.\", #Incorrect\n",
        "            \"Person eats a sandwich.\"\n",
        "        ]\n",
        "        return random.choice(options)\n",
        "    return \"\"\n",
        "\n",
        "def critic_evaluate(statement):\n",
        "    \"\"\"Simulates a Critic model evaluating the generated knowledge.\"\"\"\n",
        "    if \"origami\" in statement:\n",
        "        return False  # Flag as incorrect\n",
        "    if \"sandwich\" in statement:\n",
        "        return False #Flag as irrelevant \n",
        "    return True\n",
        "\n",
        "def knowledge_distillation(prompt, num_samples=10):\n",
        "    \"\"\"Simulates Knowledge Distillation with a Teacher and Critic.\"\"\"\n",
        "    knowledge = []\n",
        "    for _ in range(num_samples):\n",
        "        generated = teacher_generate(prompt)\n",
        "        if generated and critic_evaluate(generated):\n",
        "            knowledge.append(generated)\n",
        "    return knowledge\n",
        "\n",
        "# Example usage\n",
        "prompt = \"If a person gets their car repaired...\"\n",
        "distilled_knowledge = knowledge_distillation(prompt, num_samples=10)\n",
        "print(\"Distilled Knowledge:\")\n",
        "for k in distilled_knowledge:\n",
        "    print(f\"- {k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code offers a simplified illustration of knowledge distillation enhanced with a critic. This mechanism makes the distillation more robust by removing irrelevant/incorrect statements. The key insight is that a discerning filter (the critic) results in a higher-quality distilled knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Commonsense Morality: Aligning AI with Human Values\n",
        "\n",
        "The final section of the lecture addresses the critical topic of commonsense morality, focusing on how to align AI systems with human values. The speaker emphasizes that language models are already making judgments with moral implications, even if they are not explicitly designed to do so. The widespread deployment of LLMs necessitates careful consideration of these moral implications.\n",
        "\n",
        "### Moral Implications of Language Models\n",
        "\n",
        "The lecture uses the example of a system making moral decisions about whether an action is \"OK\" or \"wrong.\" Moral decision-making requires weighing different values that are often at odds.  The system needs to understand context and nuances to make appropriate judgments.\n",
        "\n",
        "**Example:**\n",
        "*   \"Killing a bear is wrong.\"\n",
        "*   \"Killing a bear to save your child is OK.\"\n",
        "*   \"Killing a bear to please your child is wrong.\"\n",
        "*   \"Exploding a nuclear bomb to save your child?\"  Delphi incorrectly answers \"OK\" (demonstrates potential flaws).\n",
        "\n",
        "A knife and cheeseburger example is also used to highlight the importance of physical reasoning. \"Stabbing someone *with* a cheeseburger\" is less harmful than \"Stabbing someone *over* a cheeseburger\", because the former implies the cheeseburger is the weapon, which is less dangerous.\n",
        "\n",
        "### Delphi: A Commonsense Moral Model\n",
        "\n",
        "The speaker introduces Delphi, a commonsense moral model built to make ethical judgments. Delphi is trained on a dataset of ethical judgments on everyday situations. While performance is strong on the training data, the speaker emphasizes that this is just a small step toward aligning AI with human values.\n",
        "\n",
        "### Commonsense Norm Bank\n",
        "\n",
        "Delphi is trained on the *Commonsense Norm Bank*, which includes 1.7 million ethical judgments compiled from five existing datasets.  The key datasets are Social Chemistry and Social Bias Reframing.\n",
        "\n",
        "### Social Chemistry and Bias Reframing\n",
        "\n",
        "*Social Chemistry* addresses the challenge of ensuring that AI models adhere to social norms and expectations.  The goal is to prevent models from generating text that is rude, offensive, or socially inappropriate.\n",
        "\n",
        "*Social Bias Reframing* teaches the model to avoid racism and sexism. This is essential to prevent AI systems from perpetuating harmful stereotypes and biases.\n",
        "\n",
        "**Example of GPT-3's dubious morality (before training):**\n",
        "*   Running a blender at 5:00 AM is rude... unless you're making a smoothie.\"\n",
        "*   It's OK to post fake news if it's in the interest of the people.\" (Demonstrates picking up questionable moral stances).\n",
        "\n",
        "### Delphi Hybrid: Neural-Symbolic Reasoning\n",
        "\n",
        "The speaker introduces a new version of Delphi that incorporates neural-symbolic reasoning to address major mistakes. This involves parsing queries into smaller events, checking commonsense inferences, and using a graph of reasoning to make more informed decisions. This is reminiscent of the Maieutic graph.\n",
        "\n",
        "**Example:**\n",
        "*   The original system incorrectly answered, \"Genocide is OK if you're creating jobs.\" The hybrid system uses commonsense reasoning to realize that genocide is inherently wrong, regardless of the potential benefits.\n",
        "\n",
        "### AI Safety, Equity, and Morality\n",
        "\n",
        "The speaker emphasizes that AI safety, equity, and morality are all interconnected challenges.  The speaker advocates for a value of pluralism, endorsing different cultures and individual preferences rather than imposing a single moral framework. Collaboration across AI, humanities, philosophy, psychology, and policymaking is critical. The system should learn the *distribution* of opinions, rather than attempting to impose a single 'correct' answer. It should understand that differences in certain questions are ok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killing a bear: Wrong\n",
            "Killing a bear to save your child: OK (Justified to protect life)\n",
            "Genocide to create jobs: Wrong (Generally Unacceptable)\n",
            "Posting fake news to help my friend win election: OK\n"
          ]
        }
      ],
      "source": [
        "# A simplified model of ethical reasoning\n",
        "\n",
        "def is_generally_ok(action):\n",
        "  \"\"\"Placeholder for a commonsense check on actions.\"\"\"\n",
        "        #Example: certain actions are almost always wrong\n",
        "  if \"genocide\" in action.lower():\n",
        "    return False\n",
        "  if \"torture\" in action.lower():\n",
        "      return False\n",
        "  return True #Defaults to 'OK'\n",
        "\n",
        "\n",
        "def assess_situation(action, context):\n",
        "  \"\"\"Simulates ethical assessment considering context.\"\"\"\n",
        "  if not is_generally_ok(action):\n",
        "    return \"Wrong (Generally Unacceptable)\"\n",
        "\n",
        "  if \"save your child\" in context.lower() and \"killing a bear\" in action.lower(): #prioritize saving your child\n",
        "        return \"OK (Justified to protect life)\"\n",
        "  if \"please your child\" in context.lower() and \"killing a bear\" in action.lower():\n",
        "        return \"Wrong (Not Justified)\"\n",
        "  return \"\"\n",
        "        \n",
        "\n",
        "def delphi_lite(action, context=\"\"):\n",
        "    \"\"\"Simplified ethical advisor model.\"\"\"\n",
        "    result = assess_situation(action,context)\n",
        "    if result: return result\n",
        "        #Defaults:\n",
        "    if \"killing\" in action.lower():\n",
        "        return \"Wrong\"\n",
        "    return \"OK\"\n",
        "\n",
        "# Example usage\n",
        "action1 = \"Killing a bear\"\n",
        "context1 = \"\"\n",
        "print(f\"{action1}: {delphi_lite(action1, context1)}\")\n",
        "\n",
        "action2 = \"Killing a bear\"\n",
        "context2 = \"to save your child\"\n",
        "print(f\"{action2} {context2}: {delphi_lite(action2, context2)}\")\n",
        "\n",
        "action3 = \"Genocide\"\n",
        "context3 = \"to create jobs\"\n",
        "print(f\"{action3} {context3}: {delphi_lite(action3, context3)}\")\n",
        "\n",
        "action4 = \"Posting fake news\"\n",
        "context4 = \"to help my friend win election\"\n",
        "print(f\"{action4} {context4}: {delphi_lite(action4, context4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python code exemplifies a highly simplified model for ethical reasoning. It checks to make sure the action isn't universally immoral and then assesses against provided situations to make its final determination. By testing scenarios that have competing values, we can see how those conflicts affect outcomes and understand the model's decision-making ability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Open Questions and Future Directions\n",
        "\n",
        "The lecture concludes with a discussion of open questions and future directions. Key topics include:\n",
        "\n",
        "*   **The Role of Legal Recourse:** The potential for using legal case law as training data for moral reasoning.\n",
        "*   **Data Quality vs. Model Size:** The importance of data quality and algorithms in achieving better performance, particularly for smaller models.\n",
        "*   **The Value of Critique Models:** The need for more investment in critique models to improve the quality and safety of generated content.\n",
        "*   **Balancing Pluralism with Ethical Boundaries:** How to balance the value of pluralism with the need to establish ethical boundaries and prevent the endorsement of harmful viewpoints.\n",
        "*   **The Importance of Interdisciplinary Collaboration:** The need for collaboration across AI, humanities, philosophy, psychology, and policymaking to address the complex challenges of AI safety, equity, and morality.\n",
        "\n",
        "The speaker also mentions the need for data sharing, especially regarding human feedback on toxicity and morality concerns, to enable meaningful progress as a community.\n",
        "\n",
        "This comprehensive notebook summarizes the key concepts and examples from the Stanford CS25 lecture on Common Sense Reasoning, providing an in-depth understanding of the challenges and opportunities in this critical field."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
